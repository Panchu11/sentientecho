#!/usr/bin/env python3
"""
Direct query testing for SentientEcho agent components.
Test the core functionality without the Sentient Agent Framework.
"""

import asyncio
import sys
import os
import time
from datetime import datetime

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from providers.ai_provider import AIProvider
from providers.reddit_provider import RedditProvider
from providers.twitter_provider import TwitterProvider
from providers.jina_provider import JinaProvider
from processors.query_processor import QueryProcessor
from processors.post_processor import PostProcessor
from config import get_settings


async def test_query_processing(query: str, query_name: str):
    """Test query processing and content retrieval."""
    print(f"\n{'='*80}")
    print(f"üîç Testing: {query_name}")
    print(f"üìù Query: {query}")
    print('='*80)
    
    start_time = time.time()
    
    try:
        settings = get_settings()
        
        # Initialize providers
        print("üîß Initializing providers...")
        ai_provider = AIProvider(
            api_key=settings.fireworks_api_key,
            model_id=settings.fireworks_model_id
        )
        
        reddit_provider = RedditProvider(max_results=5)
        
        twitter_provider = TwitterProvider(
            max_results=5,
            serper_api_key=settings.serper_api_key
        )
        
        jina_provider = JinaProvider(api_key=settings.jina_ai_api_key)
        
        # Initialize processors
        query_processor = QueryProcessor(ai_provider)
        post_processor = PostProcessor(ai_provider, jina_provider)
        
        print("‚úÖ All providers initialized")
        
        # Step 1: Process query
        print("\nüß† Step 1: Processing query with AI...")
        processed_query = await query_processor.process_query(query)
        
        print(f"   üéØ Keywords: {processed_query.keywords}")
        print(f"   üîç Search Reddit: {processed_query.search_reddit}")
        print(f"   üê¶ Search Twitter: {processed_query.search_twitter}")
        print(f"   üìã Intent: {processed_query.intent}")
        print(f"   ‚è±Ô∏è Time range: {processed_query.filters.time_range}")
        
        # Step 2: Search Reddit
        reddit_posts = []
        if processed_query.search_reddit:
            print("\nüì± Step 2: Searching Reddit...")
            try:
                reddit_posts = await reddit_provider.search_posts(
                    keywords=processed_query.keywords[:3],  # Limit keywords
                    time_range=processed_query.filters.time_range
                )
                print(f"   ‚úÖ Found {len(reddit_posts)} Reddit posts")
                
                for i, post in enumerate(reddit_posts[:3], 1):
                    print(f"   {i}. r/{post.metadata.get('subreddit', 'unknown')} - {post.author}")
                    print(f"      üìù {post.content[:100]}...")
                    print(f"      üëç Score: {post.metadata.get('score', 0)}")
                    
            except Exception as e:
                print(f"   ‚ùå Reddit search failed: {e}")
        
        # Step 3: Search Twitter
        twitter_posts = []
        if processed_query.search_twitter:
            print("\nüê¶ Step 3: Searching Twitter...")
            try:
                twitter_posts = await twitter_provider.search_posts(
                    keywords=processed_query.keywords[:3],  # Limit keywords
                    time_range=processed_query.filters.time_range
                )
                print(f"   ‚úÖ Found {len(twitter_posts)} Twitter posts")
                
                for i, post in enumerate(twitter_posts[:3], 1):
                    print(f"   {i}. {post.author}")
                    print(f"      üìù {post.content[:100]}...")
                    print(f"      üí´ Engagement: {post.engagement_score}")
                    
            except Exception as e:
                print(f"   ‚ùå Twitter search failed: {e}")
        
        # Step 4: Process and rank posts
        all_posts = reddit_posts + twitter_posts
        if all_posts:
            print(f"\n‚ö° Step 4: Processing {len(all_posts)} posts...")
            try:
                processed_posts = await post_processor.process_posts(
                    posts=all_posts,
                    query=query,
                    max_posts=5
                )
                
                print(f"   ‚úÖ Processed and ranked {len(processed_posts)} posts")
                
                print("\nüèÜ Top Results:")
                for i, post in enumerate(processed_posts[:3], 1):
                    print(f"   {i}. {post.source} - {post.author}")
                    print(f"      üìù {post.content[:80]}...")
                    print(f"      üéØ Relevance: {post.relevance_score:.2f}")
                    print(f"      üòä Sentiment: {post.sentiment}")
                    if hasattr(post, 'jina_relevance_score'):
                        print(f"      üß† Jina Score: {post.jina_relevance_score:.2f}")
                    if post.summary:
                        print(f"      üìÑ Summary: {post.summary[:60]}...")
                
            except Exception as e:
                print(f"   ‚ùå Post processing failed: {e}")
        else:
            print("   ‚ö†Ô∏è No posts found to process")
        
        # Step 5: Generate final response
        if processed_posts:
            print("\nüìù Step 5: Generating AI response...")
            try:
                # Create a simple response format
                response_parts = [
                    f"## üìä Found {len(processed_posts)} relevant posts about '{query}'\n"
                ]
                
                for i, post in enumerate(processed_posts[:3], 1):
                    response_parts.append(f"### {i}. {post.source} Post")
                    response_parts.append(f"**Author**: {post.author}")
                    response_parts.append(f"**Content**: {post.content[:200]}...")
                    response_parts.append(f"**Relevance**: {post.relevance_score:.2f}/1.0")
                    response_parts.append(f"**Sentiment**: {post.sentiment}")
                    if post.summary:
                        response_parts.append(f"**AI Summary**: {post.summary}")
                    response_parts.append("")
                
                final_response = "\n".join(response_parts)
                print("   ‚úÖ Generated final response")
                print(f"\nüìÑ Final Response Preview:")
                print(final_response[:500] + "..." if len(final_response) > 500 else final_response)
                
            except Exception as e:
                print(f"   ‚ùå Response generation failed: {e}")
        
        # Cleanup
        await jina_provider.close()
        
        elapsed = time.time() - start_time
        print(f"\n‚è±Ô∏è Total processing time: {elapsed:.2f} seconds")
        
        return len(all_posts) > 0
        
    except Exception as e:
        print(f"üí• Query processing failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Main testing function."""
    print("üöÄ Starting Direct SentientEcho Query Testing")
    print("   (Bypassing Sentient Agent Framework for core functionality testing)\n")
    
    # Test queries
    test_queries = [
        ("Programming Discussion", "What do people think about Python vs JavaScript for beginners?"),
        ("Reddit Focus", "Latest discussions on r/MachineLearning about AI breakthroughs"),
        ("Twitter Focus", "Twitter sentiment about the new iPhone release"),
        ("Gaming", "How is the gaming community reacting to the latest AAA game releases?"),
        ("General Tech", "Best productivity tools for remote work"),
    ]
    
    successful_queries = 0
    total_queries = len(test_queries)
    
    for i, (query_name, query_text) in enumerate(test_queries, 1):
        print(f"\nüéØ Test {i}/{total_queries}")
        success = await test_query_processing(query_text, query_name)
        
        if success:
            successful_queries += 1
            print(f"   ‚úÖ Query '{query_name}' - SUCCESS")
        else:
            print(f"   ‚ùå Query '{query_name}' - FAILED")
        
        # Small delay between queries
        if i < total_queries:
            print("   ‚è≥ Waiting 5 seconds before next query...")
            await asyncio.sleep(5)
    
    # Final summary
    print(f"\n{'='*80}")
    print("üéâ DIRECT TESTING COMPLETE")
    print('='*80)
    print(f"üìä Results Summary:")
    print(f"   ‚úÖ Successful queries: {successful_queries}/{total_queries}")
    print(f"   üìà Success rate: {(successful_queries/total_queries)*100:.1f}%")
    
    if successful_queries == total_queries:
        print(f"\nüéâ ALL QUERIES SUCCESSFUL! Core functionality is working perfectly!")
    elif successful_queries >= total_queries * 0.8:
        print(f"\n‚ú® MOSTLY SUCCESSFUL! Core functionality is working well.")
    elif successful_queries >= total_queries * 0.5:
        print(f"\n‚ö†Ô∏è PARTIALLY WORKING. Some components have issues.")
    else:
        print(f"\n‚ùå SIGNIFICANT ISSUES. Core functionality needs investigation.")
    
    print(f"\nüîß Component Status:")
    print(f"   üß† AI Provider (Sentient Dobby): {'‚úÖ Working' if successful_queries > 0 else '‚ùå Issues'}")
    print(f"   üì± Reddit Provider: {'‚úÖ Working' if successful_queries > 0 else '‚ùå Issues'}")
    print(f"   üê¶ Twitter Provider: {'‚úÖ Working' if successful_queries > 0 else '‚ùå Issues'}")
    print(f"   üéØ Query Processing: {'‚úÖ Working' if successful_queries > 0 else '‚ùå Issues'}")
    print(f"   üìä Post Processing: {'‚úÖ Working' if successful_queries > 0 else '‚ùå Issues'}")
    
    return successful_queries >= total_queries * 0.6


if __name__ == "__main__":
    success = asyncio.run(main())
    if not success:
        sys.exit(1)
